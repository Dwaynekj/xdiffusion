# Imagen Video
#
# Configuration file for implementation of "Imagen Video: High Definition Video Generation with Diffusion Models",
# (https://arxiv.org/abs/2210.02303).
#
# This is the base diffusion model that generates videos at 8x16x16. It uses a Space-Time Factorized
# Unet (from Video Diffusion Models in this case) with the v-parameterization.
diffusion:
  parameterization: "v"
  # Sampling section determines the size of the sampled output
  # from the model, and defines the sampling method used
  sampling:
    output_channels: 1
    output_spatial_size: 16
    output_frames: 8
    target: xdiffusion.samplers.ancestral.AncestralSampler
    params: {}

  # The noise scheduler to use with the forward diffusion process.
  noise_scheduler:
    target: xdiffusion.scheduler.ContinuousNoiseScheduler
    params:
      num_scales: 1024
      logsnr_schedule: "cosine"
      loss_type: "l2"
      logsnr_min: -20
      logsnr_max: 20
  importance_sampler:
    target: xdiffusion.importance_sampling.UniformSampler
    params:
      num_timesteps: 1024
  # A preprocessor to use with the context before sending to the score network.
  context_preprocessing:
      # The Prompts Preprocessor converts the list of text prompts in the context
      # into a batch of text tokens of shape (B, text_context_size)
      - target: xdiffusion.context.T5TextPromptsPreprocessor
        params:
          # The max length of the text token sequence
          max_length: 77
          # The name of the T5 text model to use
          model_name: "google/t5-v1_1-base"
  # A preprocessor for input to the model.
  input_preprocessing:
    target: xdiffusion.context.IgnoreInputPreprocessor
    params: {}
  # Setting for classifier free guidance.
  classifier_free_guidance:
    # Classifier-free guidance scale, where the value is >= 1.0
    classifier_free_guidance: 1.0
    # Unconditional guidance probability
    unconditional_guidance_probability: 0.2
    # The context signals to apply guidance to.
    signals: ["text_prompts"]
    # For classifier free guidance, we need the ability to create an unconditional
    # context given the conditional context. This unconditional context needs
    # to be applied in both training and sampling, and will return a new
    # context dictionary given the original context dictionary.
    unconditional_context:
      target: xdiffusion.context.UnconditionalTextPromptsAdapter
      params: {}
  dynamic_thresholding:
    enable: True
    p: 0.99
    c: 1.7
  # Defines the score network for predicting the noise parameter
  score_network:
    target: xdiffusion.score_networks.unet_3d.Unet
    params:
      # The number of input channels to the model.
      input_channels: 1
      # The number of output channels to the model.
      output_channels: 1
      # The spatial size of the input to the model.
      input_spatial_size: 16
      # The number of frames in each batch
      input_number_of_frames: 8
      # The number of features/channels at the start of
      # the network. This defines the inner dimensions
      # of the model.
      num_features: 128
      # Resnet block channel multipliers.
      channel_multipliers: [1, 2, 2]
      # Attention resolutions
      attention_resolutions: [8]
      # The number of resnet blocks per resolution.
      num_resnet_blocks: 2
      # Use scale/shift of the GroupNorm in the timestep embedding.
      # This is also called Adaptive Group Normalization.
      use_scale_shift_norm: True
      # Perform resampling using convolutions.
      resamp_with_conv: False
      # BigGAN style resnet block to perform up/down sampling.
      resblock_updown: False
      # The type of resnet block to use
      resnet_block_type: 'biggan'
      # Dropout scale
      dropout: 0.1
      # The number of MLP layers to use on the conditioning signal
      mlp_layers: 2
      # Does the model include a learned sigma or a fixed sigma.
      is_learned_sigma: False
      # True if this is a class conditional model
      is_class_conditional: False
      # The number of classes for a class conditional model.
      # Only used if is_class_conditional=True.
      num_classes: 10
      # Additional conditioning signals for the model. The projections
      # defined here will be applied before running through the rest of the
      # score network.
      conditioning:
        # The signals (keys in the dictionary) that are available in the conditioning
        # context.
        signals: ["timestep", "text_tokens"]
        projections:
          # A projection to apply to the integer timesteps.
          timestep:
            # Defines a projection incorporating the sinusoidal position embedding.
            # Output size is (B, C, num_features * time_embedding_mult)
            target: xdiffusion.layers.embedding.InvCosTimestepEmbeddingProjection
            params:
              num_features: 128
              time_embedding_mult: 4
              max_time: 1.0
              clip_min: -20
              clip_max: 20
          # A projection to apply to the text tokens in the conditioning context.
          text_tokens:
            # Defines an embedding which goes from text tokens at the given
            # vocabulary size to text token embeddings.
            target: xdiffusion.layers.embedding.T5TextTokensToEmbedding
            params:
              model_name: "google/t5-v1_1-base"
        # The context transformer to use at the top of the score network. This transforms
        # the context with a shared set of parameters.
        context_transformer_head:
          # Timestep -> timestep embedding
          - target: xdiffusion.layers.embedding.RunProjection
            params:
              input_context_key: "logsnr_t"
              output_context_key: "timestep_embedding"
              projection_key: "timestep"
          # Projects the text tokens into text embeddings using the projection
          # defined for "text_tokens"
          - target: xdiffusion.layers.embedding.RunProjection
            params:
              input_context_key: "text_tokens"
              output_context_key: "text_embeddings"
              projection_key: "text_tokens"
          # Adds the text embeddings to the timestep projection using a pooled embedding
          # vector with Layer Norm
          - target: xdiffusion.layers.embedding.PooledTextEmbeddingsToTimestep
            params:
              text_embedding_dim: 768
              time_embedding_dim: 512
              attention_pooling_heads: 64
        # The context transformer to use at each attention layer. This transforms
        # the context with a different set of parameters at each attention layer.
        spatial_context_transformer_layer:
          target: xdiffusion.layers.attention.SpatialCrossAttention
          params:
            context_dim: 768
            # The number of heads in the attention layer.
            heads: 4
            # The dimensionality of each attention head.
            dim_head: 64
            # Dropout probability to use in training.
            dropout: 0.1
            # Imagen suggested layer norm at the cross attention layer helps.
            context_layer_norm: True
            # The input dimension of the context projection is the text embedding
            # sequence length
            context_projection_input_dim: 77
            # Disables self attention and uses cross attention only.
            disable_self_attention: True
            # The context adapter for the conditioning signal
            context_adapter:
              target: xdiffusion.context.TextEmbeddingsAdapter
              params:
                # For the cross attention, we want to operate on the embedding dimension,
                # not the token sequence length.
                swap_context_channels: True
        temporal_context_transformer_layer:
          target: xdiffusion.layers.attention.TemporalSelfAttention
          params:
            # The dimensionality of the context signal coming in.
            # -1 signifies no context coming in, and this defaults to Multi-Head
            # self attention.
            context_dim: 768
            # The number of heads in the attention layer.
            heads: 4
            # The dimensionality of each attention head.
            dim_head: 64
            # Dropout probability to use in training.
            dropout: 0.1
            temporal_sequence_length: 8
            max_relative_position: 8
            # The context adapter for the conditioning signal
            context_adapter:
              target: xdiffusion.context.TextEmbeddingsAdapter
              params:
                # For the cross attention, we want to operate on the embedding dimension,
                # not the token sequence length.
                swap_context_channels: True
                # The text embedding length
                input_projection_dim: 77
                # Project to the number of frames in the sequence
                output_projection_dim: 8
# Describes the dataset used in training.
data:
  # Spatial width/height of the data input to the model.
  image_size: 16
  # Number of channels in the input data
  num_channels: 1
  # The number of classes in the dataset
  num_classes: 10
  # The number of frames in the input dataset. This could be different
  # than the number of frames that is in the data itself.
  input_number_of_frames: 8
  # Method of processing the input frames. Can be "clip", which takes
  # the first N frames, or "sample", which skips frames from the input
  # to get the required number of frames (so a source video of 30 frames, with input_number_of_frames
  # set to 15, would take every (30//15=2) frames).
  frame_processing: "sample"
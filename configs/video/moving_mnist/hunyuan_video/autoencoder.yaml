# Configuration for an Hunyuan-Video Causal 3D VAE. The VAE compresses the original
# (B,C,T+1,H,W) => (B, C', T/c_t + 1, H/c_s, W/c_s) where C' = 16, c_t = 4, c_s = 8.
# For example, this will compress a single channel source video space of
# (B, 1, 29, 64, 64) => (B, 16, 8, 8, 8)
vae_config:
  target: xdiffusion.autoencoders.hunyuan.HunyuanCausal3DVAE
  params:
    in_channels: 1
    out_channels: 1
    act_fn: "silu"
    block_out_channels: [
      128,
      256,
      512,
      512
    ]
    down_block_types: [
      "DownEncoderBlockCausal3D",
      "DownEncoderBlockCausal3D",
      "DownEncoderBlockCausal3D",
      "DownEncoderBlockCausal3D"
    ]
    up_block_types: [
      "UpDecoderBlockCausal3D",
      "UpDecoderBlockCausal3D",
      "UpDecoderBlockCausal3D",
      "UpDecoderBlockCausal3D"
    ]
    latent_channels: 16
    layers_per_block: 2
    norm_num_groups: 32
    sample_size: 64
    sample_tsize: 29
    scaling_factor: 1.0
    time_compression_ratio: 4
    spatial_compression_ratio: 8
    mid_block_add_attention: True

    # How to predict the variance. Can be "per_channel" or "uniform".
    # For moving MNIST, "per_channel" will mode collapse because the distributions
    # are too simple.
    latent_logvar: "uniform"

    # These settings are configured for a Moving MNIST version of this
    # 3D VAE. In particular, it's challenging for such a large VAE to reconstruct
    # a simple dataset like Moving MNIST, because it tends to mode collapse to 0
    # very quickly. So we focus on the reconstruction loss only at the beginning,
    # combined with a slow learning rate warmup, before layering in the additional losses.
    # One other difference is we use the L2 reconstruction loss here, rather than the L1
    # loss as specified in the paper. The L1 loss caused very quick mode collapse, regardless
    # of the warmup schedule or other hyperparameters we chose.
    loss_config:
      target: xdiffusion.autoencoders.losses.LPIPSWithDiscriminator
      params:
        rec_loss: "l2"
        # The iteration to start the percepual loss
        perceptual_start: 1000
        # The weight for the perceptual loss
        perceptual_weight: 0.1
        # The iteration to start the KL divergence loss
        kl_start: 5000
        # The weight for the KL divergence loss
        kl_weight: 1.0e-06
        # The iteration to start the discriminator
        disc_start: 10000
        # The weight for the discriminator loss
        disc_weight: 0.05
        # The number of input channels to the discriminator
        disc_in_channels: 1
        # True if the discriminator is conditional.
        disc_conditional: false
        # The iteration to start the wavelet loss
        wavelet_start: 0
        # The weight to use for the wavelet loss
        wavelet_loss_weight: 0.0
        # True if this is a 3D VAE, False otherwise.
        use_3d: True
        # True if we should use the reconstruction GAN formulation,
        # from LTX-Video.
        use_reconstruction_gan: False
        # True if we should use a learned log variance. False to use
        # the log variance of the posterior distribution.
        learned_logvar: False
        # True if we should calculate the NLL of the reconstruction loss,
        # False to use the reconstruction loss directly.
        use_nll: False

# Describes the dataset used in training.
data:
  # Spatial width/height of the data input to the model.
  image_size: 64
  # Number of channels in the input data
  num_channels: 1
  # The number of classes in the dataset
  num_classes: 10
  # The number of frames in the input dataset. This could be different
  # than the number of frames that is in the data itself. For Hunyuan-Video,
  # The compressed size goes from (T+1) -> (1 + T/c_t), so 29 -> 8
  input_number_of_frames: 29
  # Method of processing the input frames. Can be "clip", which takes
  # the first N frames, or "sample", which skips frames from the input
  # to get the required number of frames (so a source video of 30 frames, with input_number_of_frames
  # set to 15, would take every (30//15=2) frames).
  frame_processing: "clip"

# Describes training parameters, including sampling
# strategies for input data.
training:
  dataset: "video/moving_mnist"
  batch_size: 8
  gradient_accumulation_steps: 1
  mixed_precision: "bf16"
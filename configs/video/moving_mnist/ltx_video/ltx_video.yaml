# SoRA: Video generation models as world simulators
#
# Configuration file for a SoRA-based text-to-video diffusion model. Since there
# is no official SoRA code or paper, this model is based on what little information
# we can glean from public sources. We use two
# Imagen Video
#
diffusion:
  parameterization: "rectified_flow"
  # Sampling section determines the size of the sampled output
  # from the model, and defines the sampling method used
  sampling:
    output_channels: 1
    output_spatial_size: 32
    output_frames: 16
    target: xdiffusion.samplers.rectified_flow.AncestralSampler
    params: {}

  # The noise scheduler to use with the forward diffusion process.
  noise_scheduler:
    target: xdiffusion.scheduler.DiscreteRectifiedFlowNoiseScheduler
    params:
      # The number of noise scales
      steps: 1000
      # The maximum time
      max_time: 1.0
      # The distribution for sample times
      distribution: "logit-normal"
  sde:
    target: xdiffusion.sde.rectified_flow.RectifiedFlow
    params:
      N: 1000
      T: 1.0
  importance_sampler:
    target: xdiffusion.importance_sampling.UniformSampler
    params:
      num_timesteps: 1000
  # A preprocessor to use with the context before sending to the score network.
  context_preprocessing:
      # The Prompts Preprocessor converts the list of text prompts in the context
      # into a batch of text tokens of shape (B, text_context_size)
      - target: xdiffusion.layers.embedding.T5TextEmbedder
        params:
          # The max length of the text token sequence
          max_length: 77
          include_temporal: True
          # The name of the T5 text model to use
          version: "google/t5-v1_1-base"
          context_key: "text_embeddings"
  # A preprocessor for input to the model.
  input_preprocessing:
    target: xdiffusion.context.IgnoreInputPreprocessor
    params: {}
  # Setting for classifier free guidance.
  classifier_free_guidance:
    # Classifier-free guidance scale, where the value is >= 1.0
    classifier_free_guidance: 1.0
    # Unconditional guidance probability
    unconditional_guidance_probability: 0.2
    # The context signals to apply guidance to.
    signals: ["text_prompts"]
    # For classifier free guidance, we need the ability to create an unconditional
    # context given the conditional context. This unconditional context needs
    # to be applied in both training and sampling, and will return a new
    # context dictionary given the original context dictionary.
    unconditional_context:
      target: xdiffusion.context.UnconditionalTextPromptsAdapter
      params: {}
  dynamic_thresholding:
    enable: False
    p: 0.99
    c: 1.7
  # Defines the score network for predicting the noise parameter
  score_network:
    target: xdiffusion.score_networks.sora.Sora
    params:
      input_spatial_size: 32
      # Model input channels
      input_channels: 1
      # Model input number of frames
      input_number_of_frames: 16
      is_learned_sigma: False
      is_class_conditional: False
      input_size: [16, 32, 32]
      input_sq_size: 32
      patch_size: [1, 4, 4]
      hidden_size: 368
      depth: 12
      num_heads: 16
      mlp_ratio: 4.0
      class_dropout_prob: 0.1
      pred_sigma: False
      # Dropout ratio
      drop_path: 0.0
      # Number of channels in the text embeddings
      caption_channels: 768
      # Maximum number of tokens in the text embeddings
      model_max_length: 77
      qk_norm: True
      enable_flash_attn: False
      enable_layernorm_kernel: False
      only_train_temporal: False
      # Freeze the caption projection. Useful for adapting from a pretrained
      # model.
      freeze_y_embedder: False
      skip_y_embedder: False

      # Additional conditioning signals for the model. The projections
      # defined here will be applied before running through the rest of the
      # score network.
      conditioning:
        # The signals (keys in the dictionary) that are available in the conditioning
        # context.
        signals: []
        projections: {}
        # The context transformer to use at the top of the score network. This transforms
        # the context with a shared set of parameters.
        context_transformer_head:
          # Timestep -> timestep embedding
          - target: torch.nn.Identity
            params: {}
# Describes the dataset used in training.
data:
  # Spatial width/height of the data input to the model.
  image_size: 32
  # Number of channels in the input data
  num_channels: 1
  # The number of classes in the dataset
  num_classes: 10
  # The number of frames in the input dataset. This could be different
  # than the number of frames that is in the data itself.
  input_number_of_frames: 16
  # Method of processing the input frames. Can be "clip", which takes
  # the first N frames, or "sample", which skips frames from the input
  # to get the required number of frames (so a source video of 30 frames, with input_number_of_frames
  # set to 15, would take every (30//15=2) frames).
  frame_processing: "clip"

# Describes training parameters, including sampling
# strategies for input data.
training:
  # Use the batch training methodology from Flexible Diffusion Modeling
  # to train a joint conditional model of random frame/conditioning sets.
  flexible_diffusion_modeling: False
  # The masking strategy, can be "random" or "uniform".
  flexible_diffusion_modeling_method: "random"
  # Frame masking strategy, if not using FDM.
  mask_ratios:
    # Mask random number of frames in random positions.
    random: 0.05
    # Mask every other frame, starting randomly on the first or
    # second frame.
    interpolate: 0.005
    # Mask up to 1/4 contiguous frames starting in a random position.
    quarter_random: 0.005
    # Mask up to 1/4 contiguous frames starting at the beginning of the video.
    quarter_head: 0.005
    # Mask up to 1/4 contiguous frames starting at the end of the video
    quarter_tail: 0.005
    # Mask up to 1/4 contiguous frames, from both the beginning and the end,
    # independently.
    quarter_head_tail: 0.005
    # Mask a single frame, in a random position.
    image_random: 0.025
    # Mask a single frame, at the beginning of the video.
    image_head: 0.05
    # Mask a single frame, at the end of the video.
    image_tail: 0.025
    # Mask a single frame, at both the beginning and the end of the video.
    image_head_tail: 0.025
